{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Initial Load**\n",
        "\n",
        "Authenticate with Google Drive and read in our dataset."
      ],
      "metadata": {
        "id": "_hmSYf_UJwWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7JR-SxaJZ8E"
      },
      "outputs": [],
      "source": [
        "# Install any required packages.\n",
        "!pip install -U -q PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import any required libraries.\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from patsy import dmatrices\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.api import add_constant\n",
        "import statsmodels.discrete.discrete_model as sml\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "1FCLdhafJtag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with Google Drive.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "11rBPUydKJwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download our dataset from Google Drive.\n",
        "downloaded_file = drive.CreateFile({ 'id': '14RMV7CRXwwCt_9iLHenyQrB9GC5gYwul' })\n",
        "downloaded_file.GetContentFile('ChicagoCrimeRecords.csv')\n",
        "chicago_crime_records = pd.read_csv('ChicagoCrimeRecords.csv')"
      ],
      "metadata": {
        "id": "6lVagwgkKLex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Analysis & Preparation**\n",
        "Analyse and prepare our data before we attempt to train a predictive model using logistic regression."
      ],
      "metadata": {
        "id": "QIZfxkMkMpKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any NANs before we begin.\n",
        "chicago_crime_records.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "AxIBsb9OoX4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at all of the columns in our dataset.\n",
        "chicago_crime_records.columns.sort_values()"
      ],
      "metadata": {
        "id": "9gRfmxvSgv66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any columns that we are confident will not be of any use to our model.\n",
        "chicago_crime_records.drop(\n",
        "    columns = ['Block', 'Case Number', 'Date', 'Description', 'FBI Code', 'ID', 'IUCR', 'Latitude', 'Location', 'Longitude', 'Updated On', 'X Coordinate', 'Y Coordinate', 'Year'],\n",
        "    axis = 1,\n",
        "    inplace = True,\n",
        "    errors = 'ignore')"
      ],
      "metadata": {
        "id": "mHES2iQuN3_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the 'Arrest', 'Domestic', 'Location Description' and 'Primary Type' columns.\n",
        "chicago_crime_records['Arrest'] = chicago_crime_records['Arrest'].astype(int)\n",
        "\n",
        "chicago_crime_records['Domestic'] = chicago_crime_records['Domestic'].astype(int)\n",
        "\n",
        "chicago_crime_records['Location Description'] = chicago_crime_records['Location Description'].astype('category')\n",
        "chicago_crime_records['Location Description'] = chicago_crime_records['Location Description'].cat.codes\n",
        "\n",
        "chicago_crime_records['Murder'] = 0\n",
        "chicago_crime_records.loc[chicago_crime_records['Primary Type'] == 'HOMICIDE', 'Murder'] = 1"
      ],
      "metadata": {
        "id": "vcDYw2gm1mMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a correlation matrix, based on certain variables in our dataset.\n",
        "correlation_matrix = chicago_crime_records[[\n",
        "    'Arrest',\n",
        "    'Beat',\n",
        "    'Community Area',\n",
        "    'District',\n",
        "    'Domestic',\n",
        "    'Ward']].corr()\n",
        "\n",
        "correlation_matrix.style.background_gradient(cmap = 'coolwarm')"
      ],
      "metadata": {
        "id": "dIs5WT2vg0tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the V.I.F. for a collection of other variables in our dataset.\n",
        "features = chicago_crime_records[['Arrest', 'Beat', 'Community Area', 'District', 'Domestic', 'Ward']]\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = features.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(features.values, i) for i in range(len(features.columns))]\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "-UTcpc4Oi15F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on the previous results, both 'Beat' and 'District' are contributing to multicollinearity.\n",
        "# Consider dropping one or both of these variables to reduce inflation.\n",
        "chicago_crime_records.drop(\n",
        "    columns = ['Beat', 'District'],\n",
        "    axis = 1,\n",
        "    inplace = True,\n",
        "    errors = 'ignore')"
      ],
      "metadata": {
        "id": "WY8_NG02p46I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Construction/Training**\n",
        "Train a collection of logistic regression models with varying sets of features."
      ],
      "metadata": {
        "id": "iZsExFQjrXLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split up our training and testing sets.\n",
        "X = np.asarray(chicago_crime_records.drop(columns = ['Murder', 'Primary Type'], axis = 1))\n",
        "y = np.asarray(chicago_crime_records['Murder'])\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 78)\n",
        "\n",
        "# Train a logistic regression model.\n",
        "logit = sml.Logit(y_train, x_train).fit()\n",
        "print(logit.summary())"
      ],
      "metadata": {
        "id": "9pKl2aVprYfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a confusion matrix for our model.\n",
        "confusion_matrix = pd.crosstab(y_test, np.round_(logit.predict(x_test), 0), rownames = ['Actual'], colnames = ['Predicted'])\n",
        "sns.heatmap(confusion_matrix, annot = True)"
      ],
      "metadata": {
        "id": "1X9ynRRx9J-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a classification report for our model.\n",
        "print(classification_report(y_test, np.round_(logit.predict(x_test), 0)))"
      ],
      "metadata": {
        "id": "BWv6_jzM8qUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}